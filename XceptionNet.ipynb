{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185cdce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, reps, stride=1, start_with_relu=True, grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sepconv1 = SeparableConv2d(in_channels, out_channels, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.skip = None\n",
    "        if out_channels != in_channels or stride != 1:\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.rep = nn.Sequential()\n",
    "        for i in range(reps - 1):\n",
    "            self.rep.add_module('relu' + str(i+1), nn.ReLU(inplace=True))\n",
    "            self.rep.add_module('sepconv' + str(i+1), SeparableConv2d(out_channels, out_channels, 3, stride=1, padding=1))\n",
    "            self.rep.add_module('bn' + str(i+1), nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.sepconv2 = SeparableConv2d(out_channels, out_channels, 3, stride=stride, padding=1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x = self.sepconv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.rep(x)\n",
    "\n",
    "        x = self.sepconv2(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x += skip\n",
    "        return x\n",
    "\n",
    "class XceptionNet(nn.Module):\n",
    "   \n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(XceptionNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.block1 = Block(64, 128, 2, 2, start_with_relu=False, grow_first=True)\n",
    "        self.block2 = Block(128, 256, 2, 2, start_with_relu=True, grow_first=True)\n",
    "        self.block3 = Block(256, 728, 2, 2, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        self.middle_flow = nn.Sequential(*[Block(728, 728, 3, 1, start_with_relu=True, grow_first=True) for _ in range(8)])\n",
    "\n",
    "        # Exit Flow\n",
    "        self.block12 = Block(728, 1024, 2, 2, start_with_relu=True, grow_first=False)\n",
    "        \n",
    "        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "        \n",
    "        self.conv4 = SeparableConv2d(1536, 2048, 3, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.conv1(x); x = self.bn1(x); x = self.relu(x)\n",
    "        x = self.conv2(x); x = self.bn2(x); x = self.relu(x)\n",
    "        x = self.block1(x); x = self.block2(x); x = self.block3(x)\n",
    "\n",
    "        \n",
    "        x = self.middle_flow(x)\n",
    "\n",
    "       \n",
    "        x = self.block12(x)\n",
    "        x = self.conv3(x); x = self.bn3(x); x = self.relu(x)\n",
    "        x = self.conv4(x); x = self.bn4(x); x = self.relu(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class FaceForensicsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []  # 0 pour Original (Réel), 1 pour Deepfake (Faux)\n",
    "\n",
    "        for label_idx, folder_name in enumerate(['Original', 'Deepfake']):\n",
    "            current_dir = os.path.join(data_root, folder_name)\n",
    "            if os.path.exists(current_dir):\n",
    "                for filename in os.listdir(current_dir):\n",
    "                    if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                        self.image_paths.append(os.path.join(current_dir, filename))\n",
    "                        self.labels.append(label_idx)\n",
    "        \n",
    "        if not self.image_paths:\n",
    "            raise FileNotFoundError(f\"Aucune image trouvée dans {data_root}/Original ou {data_root}/Deepfake. Vérifiez le chemin.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\" Entraîne et valide le modèle en enregistrant les métriques. \"\"\"\n",
    "    best_acc = 0.0\n",
    "    # Dictionnaires pour stocker l'historique des métriques\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f'\\n--- Epoch {epoch+1}/{num_epochs} ---')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()   \n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "\n",
    "            for inputs, labels in tqdm(data_loader, desc=f'Phase {phase}'):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc)\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc)\n",
    "\n",
    "            print(f'{phase.capitalize()} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), 'best_xception_deepfake_detector.pth')\n",
    "                print(f\"Meilleur modèle sauvegardé avec précision: {best_acc:.4f}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Temps écoulé pour l'époque {epoch+1}: {(end_time - start_time):.2f} secondes\")\n",
    "    \n",
    "    return model, history # Retourne le modèle et l'historique\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    print(\"\\n--- Évaluation des performances du modèle ---\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Évaluation'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', pos_label=1) # Deepfake = 1\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', pos_label=1)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', pos_label=1)\n",
    "\n",
    "    print(\"\\n--- Métriques de Performance Finales ---\")\n",
    "    print(f\"Accuracy (Précision globale): {accuracy:.4f}\")\n",
    "    print(f\"Precision (Deepfake): {precision:.4f}\")\n",
    "    print(f\"Recall (Sensibilité Deepfake): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Deepfake): {f1:.4f}\")\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1}\n",
    "\n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Perte d\\'entraînement')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-', label='Perte de validation')\n",
    "    plt.title('Évolution de la Perte (Loss) par Époque')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], 'b-', label='Précision d\\'entraînement')\n",
    "    plt.plot(epochs, history['val_acc'], 'r-', label='Précision de validation')\n",
    "    plt.title('Évolution de la Précision (Accuracy) par Époque')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "   \n",
    "    DATA_ROOT = 'path/to/your/FaceForensics_Extracted_Dataset' # REMPLACEZ CE CHEMIN\n",
    "    BATCH_SIZE = 16 \n",
    "    NUM_EPOCHS = 15 \n",
    "    LEARNING_RATE = 0.0001\n",
    "    INPUT_SIZE = 299 \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Utilisation du périphérique : **{device}**\")\n",
    "\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) \n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(\"\\nChargement des données...\")\n",
    "    try:\n",
    "        full_dataset = FaceForensicsDataset(DATA_ROOT, transform=data_transforms['train'])\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erreur de chargement: {e}\")\n",
    "        print(\"Veuillez vérifier que le chemin DATA_ROOT pointe vers le dossier contenant 'Original' et 'Deepfake'.\")\n",
    "        exit()\n",
    "\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = int(0.1 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    val_dataset.dataset.transform = data_transforms['test']\n",
    "    test_dataset.dataset.transform = data_transforms['test']\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(f\"Taille Train: {len(train_dataset)}, Taille Val: {len(val_dataset)}, Taille Test: {len(test_dataset)}\")\n",
    "\n",
    "    model = XceptionNet(num_classes=1000) \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2) \n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    \n",
    "    trained_model, training_history = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "   \n",
    "    print(\"\\n--- Affichage des graphiques d'évolution ---\")\n",
    "    plot_metrics(training_history)\n",
    "    # \n",
    "\n",
    "    \n",
    "    try:\n",
    "        trained_model.load_state_dict(torch.load('best_xception_deepfake_detector.pth'))\n",
    "        print(\"\\nChargement du meilleur modèle sauvegardé pour l'évaluation finale.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nAvertissement: Le meilleur modèle n'a pas été trouvé. Utilisation de l'état final du modèle.\")\n",
    "\n",
    "    final_metrics = evaluate_model(trained_model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
